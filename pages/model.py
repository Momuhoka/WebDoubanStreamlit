import os
import pickle
import time
import streamlit as st
import numpy as np
import pandas as pd
import thulac
from stqdm import stqdm

from data.modules import diy_menu, pages_dict

# è®¾ç½®å…¨å±€å±æ€§
st.set_page_config(
    page_title='æ¨¡å‹è®­ç»ƒ',
    page_icon='ğŸ§Š',
    layout='wide',
    initial_sidebar_state='collapsed'
)
st.spinner("è½½å…¥jiebaåˆ†è¯ç¼“å­˜...")
import jieba

st.spinner("è½½å…¥tensorflowæ¨¡å‹åº“...")
import tensorflow as tf

# é¡µé¢èœå•
diy_menu(_page="æ¨¡å‹", _page_dict=pages_dict)

EAGER_MODE = ":green[TRUE]" if tf.executing_eagerly() else ":red[FALSE]"
GPU_AVAILABLE = ":green[AVAILABLE]" if tf.config.list_physical_devices('GPU') else ":red[NOT AVAILABLE]"
tc1, tc2 = st.columns(spec=[0.6, 0.4])
with tc1:
    st.markdown(f'''## **TensorFlowç‰ˆæœ¬:** {tf.__version__}   
> **Eageræ¨¡å¼:** {EAGER_MODE}   
> **GPUæ”¯æŒ:** {GPU_AVAILABLE}''')
with tc2:
    t1, t2 = st.columns(spec=2)
    # å…¨å±€é€‰æ‹©
    with st.container():
        with t1:
            st.markdown("**é¡µé¢è®¾ç½®**")
            train_module = st.toggle("æ¨¡å‹è®­ç»ƒ", value=True)
            use_module = st.toggle("æ¨¡å‹ä½¿ç”¨", value=False)
            show_module = st.toggle("æ•°æ®å±•ç¤º", value=False)
        with t2:
            threadings = st.number_input("> **TensorFlowçº¿ç¨‹æ•°**",
                                         help='''æœ€å¤§çº¿ç¨‹æ•°å¯èƒ½ä¼šå—åˆ°å…¶ä»–å› ç´ çš„é™åˆ¶,
                                                         å¦‚å†…å­˜å¤§å°ã€ç¼“å­˜å¤§å°ã€å¹¶è¡Œç¡¬ä»¶èµ„æºç­‰.
                                                         éœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªå› ç´ ,å¹¶è¿›è¡Œå®éªŒå’Œæµ‹è¯•æ¥æ‰¾åˆ°æœ€ä½³çš„è®¾ç½®''',
                                         value=6,
                                         min_value=1)
            st.info(f"**å½“å‰çº¿ç¨‹æ•°**(6): :green[{threadings}]")

# ä¸»é¡µéƒ¨åˆ†
# è®¾ç½® TensorFlow çš„çº¿ç¨‹æ•°
os.environ["TF_NUM_INTEROP_THREADS"] = str(threadings)
os.environ["TF_NUM_INTRAOP_THREADS"] = str(threadings)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ç¦ç”¨é™¤äº†é”™è¯¯ä¿¡æ¯ä¹‹å¤–çš„æ‰€æœ‰æ—¥å¿—è¾“å‡º, åŒ…æ‹¬è­¦å‘Šä¿¡æ¯
tf.config.threading.set_inter_op_parallelism_threads(threadings)
tf.config.threading.set_intra_op_parallelism_threads(threadings)
# Tensorflowç›¸å…³åº“å¯¼å…¥
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
# è‡ªå®šä¹‰å›è°ƒå‡½æ•°
from data.keras.streamlit_callback import StreamlitLambdaCallback

# ç¼“å­˜è·¯å¾„ ./cache
cachepath = "./cache"
if not os.path.exists(cachepath):
    os.mkdir(cachepath)

# éƒ¨åˆ†æ–‡ä»¶å­˜å‚¨ä½ç½®
datapath = "./data"
if not os.path.exists(f"{datapath}/keras/saves"):
    os.makedirs(f"{datapath}/keras/saves")

# è¦è¯»å–çš„ç¼“å­˜æ–‡ä»¶
cachefile_name = "cache.csv"

# é‡æ–°è®­ç»ƒæŒ‰é’®ç¼“å­˜
if "retrain" not in st.session_state:
    st.session_state.retrain = False


@st.cache_data(ttl=300,  # Cache data for 5 min (=300 seconds)
               show_spinner="æ£€æŸ¥æ•°æ®æ–‡ä»¶çŠ¶æ€...")
def checkdatafile(path: str):
    walkers = os.listdir(path)
    filespath = [f"{path}/{walker}" for walker in walkers if not os.path.isfile(f"{path}/{walker}")]
    for _filepath in filespath:
        if not os.path.isfile(f"{_filepath}/çŸ­è¯„.xlsx"):
            return f"{_filepath}/çŸ­è¯„.xlsx", False
        if not os.path.isfile(f"{_filepath}/é•¿è¯„.xlsx"):
            return f"{_filepath}/é•¿è¯„.xlsx", False
    return filespath, True


# æ•°æ®é¢„å¤„ç†æ£€æŸ¥
def pre_data_check():
    _tokenized_data_check = os.path.isfile(f"{datapath}/keras/tokenized_data.csv")
    _tokenizer_check = os.path.isfile(f"{datapath}/keras/tokenizer.pkl")
    _sequences_check = os.path.isfile(f"{datapath}/keras/sequences.pkl")
    _padded_check = os.path.isfile(f"{datapath}/keras/padded.pkl")
    return _tokenized_data_check, _tokenizer_check, _sequences_check, _padded_check


def check_mdoel_had(path: str):
    walkers = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]
    models = {}
    for walker in walkers:
        if os.path.isfile(f"{path}/{walker}/{walker}.keras"):
            modification_time = os.path.getmtime(f"{path}/{walker}/{walker}.keras")
            hadtokenizer_check = os.path.isfile(f"{path}/{walker}/{walker}.pkl")
            models[f"{walker}"] = [time.strftime(
                "%Y-%m-%d %H:%M:%S", time.localtime(modification_time)), hadtokenizer_check]
    return models


def read_data(_type: str, path: str):
    if _type == "csv":
        _data = pd.read_csv(path, index_col=0)
    elif _type == "pkl":
        with open(path, "rb") as _file:
            _data = pickle.load(_file)
    else:
        with open(path, mode="r", encoding="utf-8") as _file:
            # è¦æ³¨æ„å›è½¦ç¬¦å·-stripä¼šåˆ é™¤å·¦å³çš„ç‰¹æ®Šå­—ç¬¦-ä¸èƒ½ä¸ºç©º!
            _data = [line.strip('\n') for line in _file.readlines()]
    return _data


@st.cache_data(show_spinner="è¯»å–æ•°æ®ä¸­...")
def read_cache(path: str):
    _tokenized_data = read_data(_type="csv", path=path)
    return _tokenized_data


@st.cache_data(show_spinner="è¯»å–æ•°æ®ä¸­...")
def read_tokenizer(path: str):
    _tokenizer = read_data(_type="pkl", path=path)
    return _tokenizer


@st.cache_data(show_spinner="è¯»å–æ•°æ®ä¸­...")
def read_sequences(path: str):
    _sequences = read_data(_type="pkl", path=path)
    return _sequences


@st.cache_data(show_spinner="è¯»å–æ•°æ®ä¸­...")
def read_padded(path: str):
    _padded = read_data(_type="pkl", path=path)
    return _padded


@st.cache_resource(show_spinner="è¯»å–æ¨¡å‹ä¸­...")
def read_model(path: str):
    _model = tf.keras.models.load_model(path)
    return _model


if train_module:
    # ä¾§è¾¹æ éƒ¨åˆ†
    if os.path.isfile(f"{datapath}/{cachefile_name}"):
        data = read_data(_type="csv", path=f"{datapath}/{cachefile_name}")
        data = data.rename(columns={"Star": "star", "Comment": "comment"})
        data = data.sample(frac=1).reset_index()  # æ‰“ä¹±
        show_all_data = data.reset_index(drop=True).rename_axis("åºåˆ—ID")
        show_all_data.index = show_all_data.index + 1  # ä» 1 èµ·å§‹
        next_check = True
    else:
        data, show_all_data = None, None
        st.error(f"**ç¼ºå°‘å¿…è¦æ–‡ä»¶{datapath}/{cachefile_name}**")
        next_check = False
    if os.path.isfile(f"{datapath}/keras/model_filterwords.txt"):
        filter_words = read_data(_type="txt", path=f"{datapath}/model_filterwords.txt")
    else:
        filter_words = None

    with st.sidebar:
        with st.form("æ¨¡å‹é…ç½®"):
            t1, t2, t3 = st.tabs(["å­—å…¸ç”Ÿæˆé…ç½®", "æ¨¡å‹å‚æ•°é…ç½®", "è¿‡æ»¤è¯é¢„è§ˆ"])
            with t1:
                vocab_size = st.number_input("è¯æ±‡è¡¨å¤§å°-(vocab_size)",
                                             help='''åœ¨æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸­,
                                             æˆ‘ä»¬é€šå¸¸å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•°å­—è¡¨ç¤º,
                                             å…¶ä¸­æ¯ä¸ªå•è¯éƒ½è¢«æ˜ å°„åˆ°ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°å€¼.
                                             è¿™ä¸ªæ•´æ•°å€¼çš„èŒƒå›´å°±æ˜¯è¯æ±‡è¡¨çš„å¤§å°,
                                             ä¹Ÿå°±æ˜¯vocab_size''',
                                             value=10000,
                                             min_value=1)
                embedding_dim = st.number_input("è¯åµŒå…¥ç»´åº¦-(embedding_dim)",
                                                help='''åœ¨NLPä»»åŠ¡ä¸­,é€šå¸¸ä¼šä½¿ç”¨é¢„è®­ç»ƒçš„è¯åµŒå…¥æ¨¡å‹
                                                (å¦‚Word2Vecã€GloVeæˆ–BERT)æ¥å°†å•è¯è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º.
                                                è¿™äº›è¯åµŒå…¥æ¨¡å‹ä¼šä¸ºæ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡,
                                                è¿™ä¸ªå‘é‡çš„ç»´åº¦å°±æ˜¯embedding_dim.
                                                è¾ƒå°çš„ç»´åº¦å¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±,è€Œè¾ƒå¤§çš„ç»´åº¦å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬.
                                                ä¸€èˆ¬æ¥è¯´ï¼Œembedding_dimçš„å€¼é€šå¸¸åœ¨50åˆ°300ä¹‹é—´.''',
                                                value=50,
                                                min_value=1,
                                                max_value=300)
                max_length = st.number_input("åºåˆ—æœ€å¤§é•¿åº¦-(max_length)",
                                             help='''ç”±äºæ–‡æœ¬åºåˆ—çš„é•¿åº¦å¯èƒ½ä¸ä¸€è‡´,ä¸ºäº†æ–¹ä¾¿æ¨¡å‹å¤„ç†,
                                             æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰åºåˆ—è°ƒæ•´ä¸ºç›¸åŒçš„é•¿åº¦.max_lengthæŒ‡å®šäº†æ–‡æœ¬åºåˆ—çš„æœ€å¤§é•¿åº¦,
                                             è¶…è¿‡è¿™ä¸ªé•¿åº¦çš„åºåˆ—å°†è¢«æˆªæ–­,ä¸è¶³è¿™ä¸ªé•¿åº¦çš„åºåˆ—å°†è¢«å¡«å…….''',
                                             value=100,
                                             min_value=10)
                oov_tok = st.text_input("ç‰¹æ®Šæ ‡è®°-(token)",
                                        help='''oov_tok æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­å¸¸ç”¨çš„ä¸€ä¸ªç‰¹æ®Šæ ‡è®°(token),
                                         ç”¨äºè¡¨ç¤º'æœªçŸ¥å•è¯'(Out-Of-Vocabulary word), ä¸€èˆ¬ä¸éœ€è¦ä¿®æ”¹.''',
                                        value="<OOV>")
                with st.expander(":orange[å½“å‰å­—å…¸å‚æ•°:]", expanded=True):
                    st.markdown(f'''
                    **è¯æ±‡è¡¨å¤§å°**(10000): :green[{vocab_size}]  
                    **è¯åµŒå…¥ç»´åº¦**(16): :green[{embedding_dim}]  
                    **åºåˆ—æœ€å¤§é•¿åº¦**(100): :green[{max_length}]    
                    **æœªçŸ¥å•è¯ç‰¹æ®Šæ ‡è®°**(<OOV>): :blue[{oov_tok}]
                    ''')
            with t2:
                batch_size = st.number_input("æ¯æ‰¹æ¬¡æ ·æœ¬æ•°é‡-(batch_size)",
                                             help='''batch_sizeæŒ‡å®šæ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­çš„æ ·æœ¬æ•°é‡,
                                             ä½¿ç”¨æ‰¹å¤„ç†,å¯ä»¥æé«˜è®­ç»ƒçš„æ•ˆç‡å’Œé€Ÿåº¦.å¸¸è§çš„batch_sizeå€¼é€šå¸¸åœ¨8åˆ°256ä¹‹é—´,
                                             å…·ä½“å–å†³äºé—®é¢˜çš„å¤æ‚æ€§å’Œå¯ç”¨èµ„æº''',
                                             value=64,
                                             min_value=8,
                                             max_value=256)
                epochs = st.number_input("è¿­ä»£æ¬¡æ•°-(epochs)",
                                         help='''ä¸€ä¸ªepochè¡¨ç¤ºå°†æ•´ä¸ªè®­ç»ƒæ•°æ®é›†å®Œæ•´åœ°è¿‡ä¸€éçš„æ¬¡æ•°.
                                         æ ¹æ®æ•°æ®é›†çš„å¤§å°å’Œæ¨¡å‹çš„å¤æ‚æ€§æ¥é€‰æ‹©åˆé€‚çš„è¿­ä»£æ¬¡æ•°''',
                                         value=20,
                                         min_value=1)
                patience = st.number_input("ç›‘æµ‹æŒ‡æ ‡-(patience)",
                                           help='''EarlyStopping(TensorFlowä¸­çš„ä¸€ä¸ªå›è°ƒå‡½æ•°,
                                           ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°æ—©åœ(early stopping)æœºåˆ¶)çš„å‚æ•°ä¹‹ä¸€.
                                           å¦‚æœåœ¨è¿ç»­çš„patienceè½®è®­ç»ƒä¸­,ç›‘æµ‹æŒ‡æ ‡æ²¡æœ‰æ”¹å–„,è®­ç»ƒå°†ä¼šæå‰åœæ­¢.
                                           (0è¡¨ç¤ºæ²¡æœ‰å®¹å¿åº¦)''',
                                           value=min(epochs, 5),
                                           min_value=0,
                                           max_value=epochs)
                validation_split = st.select_slider("æ•°æ®åˆ’åˆ†-(validation_split)",
                                                    help='''ä»‹äº0å’Œ1ä¹‹é—´çš„æµ®ç‚¹æ•°,è¡¨ç¤ºè¦ä¿ç•™ä½œä¸ºéªŒè¯é›†çš„è®­ç»ƒæ•°æ®çš„æ¯”ä¾‹.
                                                    ä¾‹å¦‚0.2å°†ä¼šä»è®­ç»ƒæ•°æ®ä¸­éšæœºé€‰æ‹©20%çš„æ ·æœ¬ä½œä¸ºéªŒè¯é›†,è€Œå‰©ä¸‹çš„ 80% æ ·æœ¬å°†ç”¨äºè®­ç»ƒ.
                                                    åªæä¾›0.1~0.5å¯é€‰''',
                                                    options=[0.1, 0.2, 0.3, 0.4, 0.5],
                                                    value=0.2)
                with st.expander(":orange[å½“å‰æ¨¡å‹å‚æ•°:]", expanded=True):
                    st.markdown(f'''
                    **æ¯æ‰¹æ¬¡æ ·æœ¬æ•°é‡**(32): :green[{batch_size}]  
                    **è¿­ä»£æ¬¡æ•°**(100): :green[{epochs}]  
                    **ç›‘æµ‹æŒ‡æ ‡**(50): :green[{patience}]  
                    **æ•°æ®åˆ’åˆ†(è®­ç»ƒ/éªŒè¯)**(0.2): 
                    :green[{(1 - validation_split) * 100}]% **/** 
                    :blue[{validation_split * 100}]%
                    ''')
            with t3:
                if filter_words:
                    show_filter_words = pd.DataFrame(filter_words).rename(columns={0: "è‡ªå®šä¹‰è¿‡æ»¤"})
                    st.dataframe(show_filter_words, use_container_width=True, hide_index=True)
                else:
                    st.warning(f"{datapath}ä¸‹æ²¡æœ‰æ‰¾åˆ°model_filterwords.txtè¿‡æ»¤è¯æ–‡ä»¶", icon='ğŸ—‚ï¸')
            st.form_submit_button("åº”ç”¨ä¿®æ”¹", use_container_width=True)

    # å±•ç¤ºæ•°æ®
    with st.expander("**å·²æ”¶é›†æ•°æ®:** {}".format(":red[None]" if data is None else f":green[{len(data)}]")):
        st.dataframe(show_all_data, use_container_width=True)

    # # ä¸´æ—¶ç”¨0.1ä¸ç„¶è®­ç»ƒé›†å¤ªå¤§
    # data = data[:10000].reset_index(drop=True)

    tokenized_data_check, tokenizer_check, sequences_check, padded_check = pre_data_check()
    pre_check = tokenized_data_check & tokenizer_check & sequences_check & padded_check
    with st.status("**æ•°æ®é¢„å¤„ç†**", expanded=not pre_check):
        st.markdown("###### â„¹ï¸æ¨¡å‹é¡µé¢ååº”å’ŒåŠ è½½æ¯”è¾ƒç¼“æ…¢, è¯·è€å¿ƒç­‰å¾…")
        co1, co2 = st.columns(spec=2)
        with co1:
            cut_toggle = st.toggle("*ä½¿ç”¨ :orange[thulac] åˆ†è¯(é»˜è®¤ :orange[jieba] )*", value=False)
            retokenizer_button = st.button("**ä¸€é”®å¤„ç†**", type="primary", use_container_width=True)
            with st.container(border=True):
                infoholder = st.empty()
                infoholder_button = st.empty()
                cut_button = infoholder_button.button("æ•°æ®å¤„ç†", use_container_width=True, disabled=not next_check)
                if cut_button or retokenizer_button:
                    # æ¸…é™¤å¯¹åº”å‡½æ•°ç¼“å­˜
                    read_cache.clear()
                    # åˆ†è¯
                    s = time.time()
                    tokenized_data = pd.DataFrame()
                    stqdm(st_container=infoholder_button).pandas(desc="æ•°æ®å¤„ç†è¿›åº¦")
                    # ä¸‰åˆ†ç±»
                    tokenized_data['sentiment'] = data['star'].astype(np.int16).apply(
                        lambda x: 0 if x in [1, 2] else (1 if x == 3 else 2))
                    if not cut_toggle:
                        tokenized_data["tokenized_comment"] = data["comment"].astype(np.str_).progress_apply(
                            lambda x: ' '.join(jieba.cut(x)))
                    else:
                        thu = thulac.thulac(seg_only=True, filt=True)
                        tokenized_data["tokenized_comment"] = data["comment"].astype(np.str_).progress_apply(
                            lambda x: thu.cut(x, text=True))
                    e = time.time()
                    infoholder.info("**æˆåŠŸç”Ÿæˆæ•°æ®è¡¨:** {:.2f}s".format(e - s))
                    tokenized_data.to_csv(f"{datapath}/keras/tokenized_data.csv")
                    tokenized_data_check = True
                else:
                    if tokenized_data_check:
                        tokenized_data = read_cache(path=f"{datapath}/keras/tokenized_data.csv")
                        infoholder.info(f":green[**æ£€æµ‹åˆ°æ•°æ®å¤„ç†ç¼“å­˜**]: {len(tokenized_data)}")
                    else:
                        show_data = None
                        infoholder.info(f":red[**æ²¡æœ‰æ£€æµ‹åˆ°æ•°æ®å¤„ç†ç¼“å­˜(è¯·ç‚¹å‡»æ•°æ®å¤„ç†)**]", icon='ğŸš¨')
            with st.container(border=True):
                dictholder = st.empty()
                dictholder_button = st.empty()
                fit_button = dictholder_button.button("æ˜ å°„è¡¨ç”Ÿæˆ", use_container_width=True,
                                                      disabled=not tokenized_data_check)
                if fit_button or retokenizer_button:
                    # æ¸…é™¤å¯¹åº”å‡½æ•°ç¼“å­˜
                    read_tokenizer.clear()
                    dictholder_button.empty()
                    # å­—å…¸æ˜ å°„ç”Ÿæˆ-ä¾æ®è®­ç»ƒé›†-å¿…é¡»å”¯ä¸€
                    s = time.time()
                    # num_words:Noneæˆ–æ•´æ•°,å¤„ç†çš„æœ€å¤§å•è¯æ•°é‡ã€‚å°‘äºæ­¤æ•°çš„å•è¯ä¸¢æ‰
                    with st.spinner("è¯æ±‡æ˜ å°„è¡¨ç”Ÿæˆä¸­..."):
                        tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
                        # fit_on_textså: word_counts: è¯é¢‘ç»Ÿè®¡ç»“æœ, word_index: è¯å’Œindexçš„å¯¹åº”å…³ç³»
                        tokenizer.fit_on_texts(tokenized_data["tokenized_comment"].astype(np.str_))
                    e = time.time()
                    dictholder.info("**æˆåŠŸç”Ÿæˆè¯æ±‡æ˜ å°„è¡¨:** {:.2f}s".format(e - s))
                    # ä¿å­˜ Tokenizer å¯¹è±¡åˆ°æ–‡ä»¶
                    with open(f"{datapath}/keras/tokenizer.pkl", "wb") as file:
                        pickle.dump(tokenizer, file)
                    tokenizer_check = True
                else:
                    if tokenizer_check:
                        # ä»æ–‡ä»¶ä¸­åŠ è½½ Tokenizer å¯¹è±¡
                        tokenizer = read_tokenizer(path=f"{datapath}/keras/tokenizer.pkl")
                        dictholder.info(f":green[**æ£€æµ‹åˆ°è¯æ±‡æ˜ å°„è¡¨ç¼“å­˜**]: {len(tokenizer.index_word)}")
                    else:
                        tokenizer = None
                        dictholder.info(f":red[**æ²¡æœ‰æ£€æµ‹åˆ°è¯æ±‡æ˜ å°„è¡¨ç¼“å­˜(è¯·ç‚¹å‡»æ˜ å°„è¡¨ç”Ÿæˆ)**]", icon='ğŸš¨')
            with st.container(border=True):
                co1_1, co1_2 = st.columns(spec=2)
                with co1_1:
                    seqholder = st.empty()
                    seqholder_button = st.empty()
                    seq_button = seqholder_button.button("è½¬æ¢è¡¨ç”Ÿæˆ", use_container_width=True,
                                                         disabled=not tokenized_data_check)
                    if seq_button or retokenizer_button:
                        # æ¸…é™¤å¯¹åº”å‡½æ•°ç¼“å­˜
                        read_sequences.clear()
                        seqholder_button.empty()
                        s = time.time()
                        with st.spinner("è¯„è®ºè½¬æ¢è¡¨ç”Ÿæˆä¸­..."):
                            sequences = tokenizer.texts_to_sequences(
                                tokenized_data["tokenized_comment"].astype(np.str_))
                        e = time.time()
                        seqholder.info("**æˆåŠŸç”Ÿæˆè¯„è®ºè½¬æ¢è¡¨:** {:.2f}s".format(e - s))
                        with open(f"{datapath}/keras/sequences.pkl", "wb") as file:
                            pickle.dump(sequences, file)
                        sequences_check = True
                    else:
                        if sequences_check:
                            # ä»æ–‡ä»¶ä¸­åŠ è½½ sequences å¯¹è±¡
                            sequences = read_sequences(path=f"{datapath}/keras/sequences.pkl")
                            seqholder.info(f":green[**æ£€æµ‹åˆ°è¯„è®ºè½¬æ¢è¡¨ç¼“å­˜**]: {len(sequences)}")
                        else:
                            sequences = None
                            seqholder.info(f":red[**æ²¡æœ‰æ£€æµ‹åˆ°è¯„è®ºè½¬æ¢è¡¨ç¼“å­˜(è¯·ç‚¹å‡»è½¬æ¢è¡¨ç”Ÿæˆ)**]", icon='ğŸš¨')
                with co1_2:
                    padholder = st.empty()
                    padholder_button = st.empty()
                    pad_button = padholder_button.button("è¡¥å…¨è¡¨ç”Ÿæˆ", use_container_width=True,
                                                         disabled=not sequences_check)
                    if pad_button or retokenizer_button:
                        # æ¸…é™¤å¯¹åº”å‡½æ•°ç¼“å­˜
                        read_padded.clear()
                        padholder_button.empty()
                        s = time.time()
                        with st.spinner("è½¬æ¢è¡¥å…¨è¡¨ç”Ÿæˆä¸­..."):
                            # pad_sequences, å¯¹ä¸Šé¢ç”Ÿæˆçš„ä¸å®šé•¿åºåˆ—è¿›è¡Œè¡¥å…¨
                            padded = pad_sequences(sequences, maxlen=max_length, padding="post", truncating="post")
                        e = time.time()
                        padholder.info("**æˆåŠŸç”Ÿæˆè½¬æ¢è¡¥å…¨è¡¨:** {:.2f}s".format(e - s))
                        with open(f"{datapath}/keras/padded.pkl", "wb") as file:
                            pickle.dump(padded, file)
                        padded_check = True
                    else:
                        if padded_check:
                            # ä»æ–‡ä»¶ä¸­åŠ è½½ padded å¯¹è±¡
                            padded = read_padded(path=f"{datapath}/keras/padded.pkl")
                            padholder.info(f":green[**æ£€æµ‹åˆ°è½¬æ¢è¡¥å…¨è¡¨ç¼“å­˜**]: {len(padded)}")
                        else:
                            padded = None
                            padholder.info(f":red[**æ²¡æœ‰æ£€æµ‹åˆ°è½¬æ¢è¡¥å…¨è¡¨ç¼“å­˜(è¯·ç‚¹å‡»è¡¥å…¨è¡¨ç”Ÿæˆ)**]", icon='ğŸš¨')
        with co2:
            if show_module:
                t1, t2, t3, t4, t5 = st.tabs(
                    ["æ•°æ®å¤„ç†é¢„è§ˆ", "è¯æ±‡æ˜ å°„é¢„è§ˆ", "è¯„è®ºè½¬æ¢é¢„è§ˆ", "è½¬æ¢è¡¥å…¨é¢„è§ˆ", "è¡¥å…¨è¡¨ä¿¡æ¯"])
                with t1:
                    # å¤„ç†æ•°æ®å±•ç¤º
                    if tokenized_data is not None:
                        show_data = tokenized_data.rename(
                            columns={"sentiment": "æƒ…æ„Ÿ", "tokenized_comment": "åˆ†è¯"}).rename_axis("åºåˆ—ID")
                        show_data.index = show_data.index + 1  # ä»1å¼€å§‹
                    else:
                        show_data = None
                    st.dataframe(show_data, use_container_width=True)
                with t2:
                    # è¯æ±‡è¡¨å±•ç¤º
                    if tokenizer is not None:
                        show_tokenizer = pd.Series(tokenizer.index_word)  # å­—å…¸æ˜ å°„å˜dataframe
                        show_tokenizer = show_tokenizer.rename(index="æ˜ å°„è¯æ±‡").rename_axis("ç´¢å¼•")
                    else:
                        show_tokenizer = None
                    st.dataframe(show_tokenizer, use_container_width=True)
                with t3:
                    # è½¬æ¢è¡¨å±•ç¤º, ç”±äºå¤ªå¤§ç”¨å­—å…¸æ‹†åˆ†è¡¨ç¤º
                    show_seq = sequences
                    if show_seq is not None:
                        length = len(sequences)
                        seq_index = st.number_input(f"**åºåˆ—ID:** :green[1~{length}] ",
                                                    value=1,
                                                    min_value=1,
                                                    max_value=length)
                        show_seq = pd.DataFrame(sequences[seq_index - 1])
                        show_seq.index = show_seq.index + 1  # ä»1å¼€å§‹
                        show_seq = show_seq.rename(columns={0: "ç‰¹å¾å€¼"}).rename_axis("æ—¶é—´æ­¥")
                    st.dataframe(show_seq, use_container_width=True)
                with t4:
                    # è¡¥å…¨è¡¨å±•ç¤º
                    show_padded = None
                    if padded is not None:
                        show_padded = pd.DataFrame(padded).rename_axis("åºåˆ—ID")
                        show_padded.index = show_padded.index + 1
                    st.dataframe(show_padded, use_container_width=True)
                with t5:
                    # è¡¥å…¨è¡¨çŠ¶æ€å±•ç¤º
                    cp1, cp2 = st.columns(spec=2)
                    # è®¡ç®—æ¯è¡Œçš„é•¿åº¦
                    with cp1:
                        lengths = np.array([len(np.trim_zeros(row, 'b')) for row in padded])
                        show_lengths = pd.Series(lengths).rename_axis("åºåˆ—").rename(index="é•¿åº¦")
                        show_lengths.index = show_lengths.index + 1
                        st.dataframe(show_lengths, use_container_width=True)
                    # ç”Ÿæˆç»Ÿè®¡è¡¨
                    with cp2:
                        unique_lengths, counts = np.unique(lengths, return_counts=True)
                        stat_table = np.column_stack((unique_lengths, counts))
                        show_table = pd.DataFrame(stat_table) \
                            .set_index(0).rename_axis("é•¿åº¦").rename(columns={1: "å æ¯”æ•°"})
                        show_table["å æ¯”"] = show_table["å æ¯”æ•°"].apply(
                            lambda x: "{:.2f}%".format(x / len(padded) * 100))
                        st.dataframe(show_table, use_container_width=True)
            else:
                st.info("æ²¡æœ‰å¯ç”¨ **:orange[æ•°æ®å±•ç¤º]** ", icon='âš ï¸')

    next_check = tokenized_data_check & tokenizer_check & sequences_check & padded_check
    with st.container(border=True):
        model_name = st.text_input("**æ¨¡å‹åç§°:**",
                                   placeholder="å­˜åœ¨åˆ™è¯»å–æ¨¡å‹/ä¸å­˜åœ¨åˆ™æ–°å»ºæ¨¡å‹",
                                   disabled=not next_check)
        if next_check and model_name:
            if not os.path.exists(f"{datapath}/keras/saves/{model_name}"):
                os.mkdir(f"{datapath}/keras/saves/{model_name}")
            if not os.path.isfile(f"{datapath}/keras/saves/{model_name}/{model_name}.keras"):
                # æ„å»ºCNNæ¨¡å‹
                with st.spinner("æ„å»ºCNNæ¨¡å‹..."):
                    s = time.time()
                    model = Sequential([
                        Embedding(vocab_size, embedding_dim, input_length=max_length),
                        Conv1D(128, 5, activation='relu'),
                        Dropout(0.5),
                        GlobalMaxPooling1D(),
                        Dense(3, activation='softmax')
                    ])
                    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
                    e = time.time()
                st.info("**CNNæ¨¡å‹:red[æ„å»º]æˆåŠŸ:** {:.2f}s".format(e - s))
                model.save(f"{datapath}/keras/saves/{model_name}/{model_name}.keras")
            else:
                model = read_model(f"{datapath}/keras/saves/{model_name}/{model_name}.keras")
                st.info(f"**CNNæ¨¡å‹:green[è¯»å–]æˆåŠŸ:** {model_name}")

            tc1, tc2 = st.columns(spec=2)
            with st.container(border=True):
                with tc1:
                    # è·å–æ¨¡å‹å­—å…¸çš„å‚æ•°
                    model_vocab_size = model.get_layer('embedding').input_dim
                    model_embedding_dim = model.get_layer('embedding').output_dim
                    model_max_length = model.get_layer('embedding').input_length
                    # æ£€æŸ¥
                    vocab_size_check = vocab_size == model_vocab_size
                    embedding_dim_check = embedding_dim == model_embedding_dim
                    max_length_check = max_length == model_max_length
                    model_check = vocab_size_check & embedding_dim_check & max_length_check
                    padded_check = padded.shape[1] == model_max_length  # å®é™…æ·±åº¦
                    st.markdown(f'''**å­—å…¸å‚æ•°(:orange[å½“å‰]/:blue[{model_name}]):**    
> **è¯æ±‡è¡¨å¤§å°**(10000): :orange[{vocab_size}]/:blue[{model_vocab_size}] {":green[âœ…åŒ¹é…]" if vocab_size_check else ":red[âä¸åŒ¹é…]"}  
> **è¯åµŒå…¥ç»´åº¦**(16): :orange[{embedding_dim}]/:blue[{model_embedding_dim} {":green[âœ…åŒ¹é…]" if embedding_dim_check else ":red[âä¸åŒ¹é…]"}]  
> **åºåˆ—æœ€å¤§é•¿åº¦**(100): :orange[{max_length}]/:blue[{model_max_length}] {":green[âœ…åŒ¹é…]" if max_length_check else ":red[âä¸åŒ¹é…]"}  
> **{":red[âè¯·é‡æ„æ¨¡å‹]" if not model_check else (":red[âš ï¸å‚æ•°æ›´æ–°ä½†ç¼“å­˜è¿‡æ—§, è¯·åˆ·æ–°ç¼“å­˜]" if not padded_check else ":green[âœ…æ¨¡å‹å¯ä»¥è¿è¡Œ]")}**''')
                with tc2:
                    st.markdown(f'''**æ¨¡å‹è¿è¡Œå‚æ•°(:orange[å½“å‰]):**  
> **æ¯æ‰¹æ¬¡æ ·æœ¬æ•°é‡**: :orange[{batch_size}]  
> **è¿­ä»£æ¬¡æ•°**: :orange[{epochs}]  
> **ç›‘æµ‹æŒ‡æ ‡**: :orange[{patience}]  
> **æ•°æ®åˆ’åˆ†(è®­ç»ƒ/éªŒè¯)**: 
:orange[{int((1 - validation_split) * len(tokenized_data))}] **/** 
:blue[{int(validation_split * len(tokenized_data))}]''')

            with st.form("train"):
                rebuild_toggle = st.toggle("è¦†ç›–", value=False, help="è®­ç»ƒå®Œæ¯•åè¦†ç›–åŸæ¨¡å‹")
                train_button = st.form_submit_button("å¼€å§‹è®­ç»ƒ", type="primary", use_container_width=True,
                                                     disabled=not (model_check & padded_check))
            if train_button or st.session_state.retrain:
                st.spinner("æ¨¡å‹è®­ç»ƒä¸­...")
                # è¿­ä»£æ¬¡æ•° epochs - yes
                # æ¯æ¬¡å¤„ç†çš„æ‰¹æ¬¡å¤§å° batch_size - yes
                # ä¼˜åŒ–å™¨ optimizer - no
                # æ¿€æ´»å‡½æ•° Activation - no
                # æŸå¤±å‡½æ•° Loss_function
                # å¦‚æœåœ¨è¿ç»­çš„ patience è½®è®­ç»ƒä¸­,ç›‘æµ‹æŒ‡æ ‡æ²¡æœ‰æ”¹å–„,è®­ç»ƒå°†ä¼šæå‰åœæ­¢.é»˜è®¤å€¼ä¸º0,è¡¨ç¤ºæ²¡æœ‰å®¹å¿åº¦,åªè¦ç›‘æµ‹æŒ‡æ ‡æ²¡æœ‰æ”¹å–„å°±ä¼šç«‹å³åœæ­¢è®­ç»ƒ
                early_stopping = EarlyStopping(monitor='val_loss', patience=patience)
                # é™ä½å­¦ä¹ ç‡é˜²æ­¢è¿‡æ‹Ÿåˆ
                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.0001)
                # è‡ªå®šä¹‰çš„å›è°ƒå‡½æ•°, åŒ…å«å®¹å™¨æ˜¾ç¤ºå’Œè‡ªåŠ¨ä¿å­˜(è¦†ç›–/æ—¶é—´æˆ³å‘½å)
                streamlit_callback = StreamlitLambdaCallback(tokenizer_map=tokenizer,
                                                             model_name=model_name if rebuild_toggle else None)
                model.fit(padded, tokenized_data["sentiment"], epochs=epochs, batch_size=batch_size,
                          validation_split=validation_split,
                          callbacks=[streamlit_callback, reduce_lr, early_stopping])

if use_module:
    # æŸ¥è¯¢å¯ç”¨æ¨¡å‹
    models_list = check_mdoel_had(f"{datapath}/keras/saves")
    hadmodel_check = True if models_list else False
    with st.sidebar:
        with st.expander("**æ¨¡å‹é¢„è§ˆ**", expanded=not hadmodel_check):
            show_models = models_list
            if hadmodel_check:
                show_models = pd.DataFrame(show_models).T.rename(columns={0: "ä¿®æ”¹æ—¥æœŸ", 1: "æ˜ å°„è¡¨æ–‡ä»¶"}).rename_axis(
                    "å¯ç”¨æ¨¡å‹")
            st.dataframe(show_models, use_container_width=True)
        # é€‰æ‹©æ¨¡å‹
        chosen_model = st.selectbox("**é€‰æ‹©æ¨¡å‹**", models_list.keys(), disabled=not hadmodel_check)
        if hadmodel_check:
            next_check, map_saves = True, True
            if not models_list[chosen_model][1]:
                if os.path.isfile(f"{datapath}/keras/tokenizer.pkl"):
                    st.warning("æ²¡æœ‰æ£€æµ‹åˆ°å¯¹åº”çš„æ˜ å°„æ–‡ä»¶, ä½¿ç”¨å…¨å±€æ˜ å°„å¯èƒ½å‡ºç°æœªçŸ¥çš„é”™è¯¯", icon='âš ï¸')
                    map_saves = False
                else:
                    st.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ˜ å°„æ–‡ä»¶, è¯·å…ˆæŒ‰ç…§é¡ºåºç”Ÿæˆ", icon='ğŸš«')
                    next_check = False
        else:
            next_check, map_saves = False, False
        # è¯»å–æ¨¡å‹
        if hadmodel_check:
            s = time.time()
            predict_model = tf.keras.models.load_model(f"{datapath}/keras/saves/{chosen_model}/{chosen_model}.keras")
            e = time.time()
            st.success(f"**:orange[{chosen_model}]æ¨¡å‹**è¯»å–æˆåŠŸ: {(e - s):.2f}s", icon='âœ…')
        else:
            predict_model = None
        # è¯»å–æ˜ å°„è¡¨
        if next_check:
            if map_saves:
                # å¯¹åº”æ–‡ä»¶åŠ è½½ Tokenizer å¯¹è±¡
                with st.spinner("åŠ è½½å¯¹åº”æ˜ å°„è¡¨..."):
                    s = time.time()
                    predict_tokenizer = read_data(_type="pkl",
                                                  path=f"{datapath}/keras/saves/{chosen_model}/{chosen_model}.pkl")
                    e = time.time()
                st.success(f"**:orange[{chosen_model}]æ˜ å°„è¡¨**åŠ è½½æˆåŠŸ: {(e - s):.2f}s", icon='âœ…')
            else:
                # å¯¹åº”æ–‡ä»¶åŠ è½½ Tokenizer å¯¹è±¡
                with st.spinner("åŠ è½½å…¨å±€æ˜ å°„è¡¨..."):
                    s = time.time()
                    predict_tokenizer = read_data(_type="pkl", path=f"{datapath}/keras/tokenizer.pkl")
                    e = time.time()
                st.success(f"**å…¨å±€æ˜ å°„è¡¨åŠ è½½æˆåŠŸ:** {(e - s):.2f}s", icon='âœ…')
        # æ¸…ç©ºå†å²
        if st.button("æ¸…ç©ºå†å²è®°å½•", use_container_width=True, type="primary",
                     disabled=chosen_model not in st.session_state):
            del st.session_state[chosen_model][:]

    # æ¨¡å‹é¢„æµ‹è¾“å…¥
    inputwords = st.chat_input("ç°åœ¨æƒ³è¯´ç‚¹ä»€ä¹ˆ?", disabled=not next_check)
    if inputwords:
        # åˆ†è¯
        cutwords = jieba.lcut(inputwords)
        # æ˜ å°„
        predict_sequences = predict_tokenizer.texts_to_sequences([' '.join(cutwords)])
        # è¡¥å…¨
        predict_model_max_length = predict_model.get_layer('embedding').input_length  # è·å–æ¨¡å‹å­—å…¸çš„å‚æ•°
        predict_padded = pad_sequences(predict_sequences, maxlen=predict_model_max_length, padding="post",
                                       truncating="post")
        # è¾“å…¥ç„¶åè¿›è¡Œé¢„æµ‹
        predict_result = predict_model.predict(predict_padded)
        # å®šä¹‰ç±»åˆ«æ ‡ç­¾
        labels = {0: "æ¶ˆæ", 1: "ä¸­æ€§", 2: "ç§¯æ"}
        color_labels = {0: ":red[æ¶ˆæ]", 1: ":grey[ä¸­æ€§]", 2: ":green[ç§¯æ]"}
        show_result = pd.DataFrame(predict_result).rename(columns=labels)
        # æ‰¾åˆ°æœ€å¤§æ¦‚ç‡å€¼çš„ç´¢å¼•
        predict_classes = np.argmax(predict_result, axis=1)
        predict_class = predict_classes[0]
        # å­˜å‚¨å†å²
        st.session_state[chosen_model].append({"å›¾è¡¨": show_result,
                                               "è¾“å…¥": inputwords,
                                               "åˆ†è¯": '//'.join(cutwords),
                                               "ç»“è®º": color_labels[predict_class],
                                               "æ—¥æœŸ": time.strftime("%Y-%m-%d %H:%M:%S",
                                                                     time.localtime(time.time()))})
    # æ‰“å°æƒ…æ„Ÿç±»åˆ«
    if next_check:
        # å­˜å‚¨å¯¹åº”æ¨¡å‹çš„è®°å½•
        st.session_state.setdefault(chosen_model, [])
        for output in st.session_state[chosen_model]:
            with st.chat_message("ğŸ§Š"):
                with st.container(border=True):
                    st.dataframe(output["å›¾è¡¨"], use_container_width=True, hide_index=True)
                    st.success("**æƒ…æ„Ÿåˆ¤åˆ«ç»“æœ**", icon='ğŸ§Š')
                    st.info(f'''
> **è¾“å…¥:**  ***:blue[{output["è¾“å…¥"]}]***  
> **åˆ†è¯:** {output["åˆ†è¯"]}  
> **ç»“è®º:** ***{output["ç»“è®º"]}***''')
                    st.markdown(f"ğŸ“Œ: *:grey[{output['æ—¥æœŸ']}]* - ğŸ§Š: *:orange[{chosen_model}]*")
